#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‰§è¡Œæ•°æ®å¹³è¡¡çš„ä¸»è„šæœ¬

ç»“åˆä½ çš„å…·ä½“éœ€æ±‚ï¼š
- æ–°å¶å¤æ‘-æ–°å¶å¤æ‘é—¨ç¥¨: 1 -> 5 (+4)
- å¤§æ…ˆå²©-å¤§æ…ˆå²©ç´¢é“: 2 -> 5 (+3) 
- å…¶ä»–ä½é¢‘èµ„æºä¹Ÿä¼šè¢«ç›¸åº”å¢å¼º
"""

import json
import sys
import os
from advanced_data_augmentation import AdvancedDataAugmenter

def load_training_data(file_path: str):
    """åŠ è½½åŸå§‹è®­ç»ƒæ•°æ®"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def merge_enhanced_samples(original_data, enhanced_samples):
    """åˆå¹¶åŸå§‹æ•°æ®å’Œå¢å¼ºæ ·æœ¬"""
    return original_data + enhanced_samples

def analyze_final_distribution(data):
    """åˆ†ææœ€ç»ˆçš„æ•°æ®åˆ†å¸ƒ"""
    from collections import Counter
    
    resource_counts = Counter()
    
    for item in data:
        if 'output' in item:
            try:
                output_data = json.loads(item['output'])
                if 'resource_names' in output_data:
                    resources = output_data['resource_names']
                    for resource in resources:
                        resource_counts[resource] += 1
            except:
                continue
    
    print("ğŸ“Š æœ€ç»ˆæ•°æ®åˆ†å¸ƒ:")
    print("-" * 60)
    
    # é‡ç‚¹å…³æ³¨çš„ä½é¢‘èµ„æº
    focus_resources = [
        "æ–°å¶å¤æ‘-æ–°å¶å¤æ‘é—¨ç¥¨",
        "å¤§æ…ˆå²©-å¤§æ…ˆå²©ç´¢é“", 
        "çµæ –æ´-çµæ –æ´è¥¿æ¸¸é­”æ¯¯",
        "å®¿æ±Ÿå…¬å¸-æ±Ÿæ¸…æœˆè¿‘äººå®æ™¯æ¼”è‰ºé—¨ç¥¨"
    ]
    
    print("ğŸ¯ é‡ç‚¹å…³æ³¨çš„èµ„æº:")
    for resource in focus_resources:
        count = resource_counts.get(resource, 0)
        print(f"   {resource}: {count}")
    
    print(f"\nğŸ“ˆ æ‰€æœ‰èµ„æºåˆ†å¸ƒ (æ€»è®¡ {len(resource_counts)} ç§èµ„æº):")
    for resource, count in resource_counts.most_common():
        status = "âœ…" if count >= 5 else "âš ï¸"
        print(f"   {status} {resource}: {count}")

def main():
    # è¾“å…¥æ–‡ä»¶è·¯å¾„ - æ ¹æ®ä½ çš„å®é™…è·¯å¾„è°ƒæ•´
    input_files = [
        "/home/ziqiang/LLaMA-Factory/data/ocr_text_orders_08_14_test_v4.json"
    ]
    
    # å°è¯•æ‰¾åˆ°å­˜åœ¨çš„è®­ç»ƒæ•°æ®æ–‡ä»¶
    training_file = None
    for file_path in input_files:
        if os.path.exists(file_path):
            training_file = file_path
            break
    
    if not training_file:
        print("âŒ æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶ï¼Œè¯·æ£€æŸ¥è·¯å¾„:")
        for file_path in input_files:
            print(f"   {file_path}")
        return
    
    print(f"ğŸ“‚ ä½¿ç”¨è®­ç»ƒæ•°æ®æ–‡ä»¶: {training_file}")
    print("=" * 60)
    
    # åŠ è½½åŸå§‹æ•°æ®
    print("ğŸ“¥ åŠ è½½åŸå§‹è®­ç»ƒæ•°æ®...")
    original_data = load_training_data(training_file)
    print(f"   åŸå§‹æ ·æœ¬æ•°: {len(original_data)}")
    
    # ç”Ÿæˆå¢å¼ºæ ·æœ¬
    print("\nğŸ”„ ç”Ÿæˆå¢å¼ºæ ·æœ¬...")
    augmenter = AdvancedDataAugmenter()
    enhanced_samples = augmenter.generate_all_samples()
    
    # åˆå¹¶æ•°æ®
    print(f"\nğŸ”— åˆå¹¶åŸå§‹æ•°æ®å’Œå¢å¼ºæ ·æœ¬...")
    balanced_data = merge_enhanced_samples(original_data, enhanced_samples)
    print(f"   åˆå¹¶åæ ·æœ¬æ•°: {len(balanced_data)}")
    print(f"   æ–°å¢æ ·æœ¬æ•°: {len(enhanced_samples)}")
    
    # ä¿å­˜å¹³è¡¡åçš„æ•°æ®
    output_file = "balanced_training_data.json"
    print(f"\nğŸ’¾ ä¿å­˜å¹³è¡¡åçš„æ•°æ®åˆ°: {output_file}")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(balanced_data, f, ensure_ascii=False, indent=2)
    
    # åˆ†ææœ€ç»ˆåˆ†å¸ƒ
    print(f"\nğŸ“Š åˆ†ææœ€ç»ˆæ•°æ®åˆ†å¸ƒ...")
    analyze_final_distribution(balanced_data)
    
    print(f"\nğŸ‰ æ•°æ®å¹³è¡¡å®Œæˆï¼")
    print("ğŸ“‹ å»ºè®®çš„ä¸‹ä¸€æ­¥:")
    print("   1. ä½¿ç”¨ balanced_training_data.json é‡æ–°è®­ç»ƒæ¨¡å‹")
    print("   2. åœ¨éªŒè¯é›†ä¸Šæµ‹è¯•æ€§èƒ½æ”¹è¿›")
    print("   3. ç‰¹åˆ«å…³æ³¨æ–°å¶å¤æ‘ã€å¤§æ…ˆå²©ç´¢é“ç­‰ä½é¢‘èµ„æºçš„è¯†åˆ«æ•ˆæœ")

if __name__ == "__main__":
    main()
